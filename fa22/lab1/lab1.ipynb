{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f10252aa",
   "metadata": {},
   "source": [
    "# Lab 1 - Text Preprocessing and N-Gram LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6a90e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d87ab548",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_1 = \"That U.S.A. poster-print costs $12.40...\"\n",
    "text_2 = \"Hope, is the only thing stronger than fear! Hunger Games #Hope\"\n",
    "tweet = \"Don't take cryptocurrency advice from people on twitter. ðŸ˜…ðŸ‘Œ\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb1693e",
   "metadata": {},
   "source": [
    "## Text Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679ebb3a",
   "metadata": {},
   "source": [
    "### NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46276b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00792cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer, TreebankWordTokenizer, WordPunctTokenizer \n",
    "from nltk.tokenize import WhitespaceTokenizer, TweetTokenizer, MWETokenizer, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "516093b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer('(?:[A-Z]\\.)+|\\\\w-w+|\\$[\\d\\.]+|\\S+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95a6ad13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regular expression text_1:  ['That', 'U.S.A.', 'poster-print', 'costs', '$12.40...']\n",
      "\n",
      "Regular expression: text_2 ['Hope,', 'is', 'the', 'only', 'thing', 'stronger', 'than', 'fear!', 'Hunger', 'Games', '#Hope']\n",
      "\n",
      "Regular expression tweet:  [\"Don't\", 'take', 'cryptocurrency', 'advice', 'from', 'people', 'on', 'twitter.', 'ðŸ˜…ðŸ‘Œ']\n"
     ]
    }
   ],
   "source": [
    "print('Regular expression text_1: ',tokenizer.tokenize(text_1))\n",
    "print('\\nRegular expression: text_2',tokenizer.tokenize(text_2))\n",
    "print('\\nRegular expression tweet: ',tokenizer.tokenize(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "378c9039",
   "metadata": {},
   "outputs": [],
   "source": [
    "tk_TreeBank = TreebankWordTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "299f2aa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TreeBank text_1:\n",
      " ['That', 'U.S.A.', 'poster-print', 'costs', '$', '12.40', '...']\n",
      "\n",
      "TreeBank text_2:\n",
      " ['Hope', ',', 'is', 'the', 'only', 'thing', 'stronger', 'than', 'fear', '!', 'Hunger', 'Games', '#', 'Hope']\n",
      "\n",
      "TreeBank tweet:\n",
      " ['Do', \"n't\", 'take', 'cryptocurrency', 'advice', 'from', 'people', 'on', 'twitter.', 'ðŸ˜…ðŸ‘Œ']\n"
     ]
    }
   ],
   "source": [
    "print('TreeBank text_1:\\n',tk_TreeBank.tokenize(text_1))\n",
    "print('\\nTreeBank text_2:\\n',tk_TreeBank.tokenize(text_2))\n",
    "print('\\nTreeBank tweet:\\n',tk_TreeBank.tokenize(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29dada6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tk_WordPunct = WordPunctTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a04c7c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordPunct text_1:\n",
      " ['That', 'U', '.', 'S', '.', 'A', '.', 'poster', '-', 'print', 'costs', '$', '12', '.', '40', '...']\n",
      "\n",
      "WordPunct text_2:\n",
      " ['Hope', ',', 'is', 'the', 'only', 'thing', 'stronger', 'than', 'fear', '!', 'Hunger', 'Games', '#', 'Hope']\n",
      "\n",
      "WordPunct tweet:\n",
      " ['Don', \"'\", 't', 'take', 'cryptocurrency', 'advice', 'from', 'people', 'on', 'twitter', '.', 'ðŸ˜…ðŸ‘Œ']\n"
     ]
    }
   ],
   "source": [
    "print('WordPunct text_1:\\n',tk_WordPunct.tokenize(text_1))\n",
    "print('\\nWordPunct text_2:\\n',tk_WordPunct.tokenize(text_2))\n",
    "print('\\nWordPunct tweet:\\n',tk_WordPunct.tokenize(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b789061c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tk_WhiteSpace = WhitespaceTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f37c3620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WhiteSpace text_1:\n",
      " ['That', 'U.S.A.', 'poster-print', 'costs', '$12.40...']\n",
      "\n",
      "WhiteSpace text_2:\n",
      " ['Hope,', 'is', 'the', 'only', 'thing', 'stronger', 'than', 'fear!', 'Hunger', 'Games', '#Hope']\n",
      "\n",
      "WhiteSpace tweet:\n",
      " [\"Don't\", 'take', 'cryptocurrency', 'advice', 'from', 'people', 'on', 'twitter.', 'ðŸ˜…ðŸ‘Œ']\n"
     ]
    }
   ],
   "source": [
    "print('WhiteSpace text_1:\\n',tk_WhiteSpace.tokenize(text_1))\n",
    "print('\\nWhiteSpace text_2:\\n',tk_WhiteSpace.tokenize(text_2))\n",
    "print('\\nWhiteSpace tweet:\\n',tk_WhiteSpace.tokenize(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9af0c099",
   "metadata": {},
   "outputs": [],
   "source": [
    "tk_Tweet = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea6cdba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet text_1:\n",
      " ['That', 'U', '.', 'S', '.', 'A', '.', 'poster-print', 'costs', '$', '12.40', '...']\n",
      "\n",
      "Tweet text_2:\n",
      " ['Hope', ',', 'is', 'the', 'only', 'thing', 'stronger', 'than', 'fear', '!', 'Hunger', 'Games', '#Hope']\n",
      "\n",
      "Tweet tweet:\n",
      " [\"Don't\", 'take', 'cryptocurrency', 'advice', 'from', 'people', 'on', 'twitter', '.', 'ðŸ˜…', 'ðŸ‘Œ']\n"
     ]
    }
   ],
   "source": [
    "print('Tweet text_1:\\n',tk_Tweet.tokenize(text_1))\n",
    "print('\\nTweet text_2:\\n',tk_Tweet.tokenize(text_2))\n",
    "print('\\nTweet tweet:\\n',tk_Tweet.tokenize(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22c4b5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tk_MWET = MWETokenizer()\n",
    "tk_MWET.add_mwe((\"Hunger\",\"Games\"))\n",
    "tk_MWET.add_mwe((\"$\",\"12.40\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c7f98c35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MWET text_1:\n",
      " ['That', 'U.S.A.', 'poster-print', 'costs', '$_12.40', '...']\n",
      "\n",
      "MWET text_2:\n",
      " ['Hope', ',', 'is', 'the', 'only', 'thing', 'stronger', 'than', 'fear', '!', 'Hunger_Games', '#', 'Hope']\n",
      "\n",
      "MWET tweet:\n",
      " ['Do', \"n't\", 'take', 'cryptocurrency', 'advice', 'from', 'people', 'on', 'twitter', '.', 'ðŸ˜…ðŸ‘Œ']\n"
     ]
    }
   ],
   "source": [
    "print('MWET text_1:\\n',tk_MWET.tokenize(word_tokenize(text_1)))\n",
    "print('\\nMWET text_2:\\n',tk_MWET.tokenize(word_tokenize(text_2)))\n",
    "print('\\nMWET tweet:\\n',tk_MWET.tokenize(word_tokenize(tweet)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8494b94",
   "metadata": {},
   "source": [
    "### Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f9451098",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "63476cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That\n",
      "U.S.A.\n",
      "poster\n",
      "-\n",
      "print\n",
      "costs\n",
      "$\n",
      "12.40\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(text_1)\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2aabbff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hope\n",
      ",\n",
      "is\n",
      "the\n",
      "only\n",
      "thing\n",
      "stronger\n",
      "than\n",
      "fear\n",
      "!\n",
      "Hunger\n",
      "Games\n",
      "#\n",
      "Hope\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(text_2)\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e776a67d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do\n",
      "n't\n",
      "take\n",
      "cryptocurrency\n",
      "advice\n",
      "from\n",
      "people\n",
      "on\n",
      "twitter\n",
      ".\n",
      "ðŸ˜…\n",
      "ðŸ‘Œ\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(tweet)\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6757be08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English\n",
    "nlp_special = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4c4cd566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blank tokenizer : That, U.S.A., poster-print, costs, $12.40..., "
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(nlp_special.vocab)\n",
    "tokens = tokenizer(text_1)\n",
    "print(\"Blank tokenizer\",end=\" : \")\n",
    "for token in tokens:\n",
    "    print(token, end=', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8c4fde40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.symbols import ORTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ad0e7f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Special case tokenization :  Hope, ,, is, the, only, thing, stronger, than, fear, !, Hunger Games, #Hope, "
     ]
    }
   ],
   "source": [
    "special_case_1 = [{ORTH: \"Hunger Games\"}]\n",
    "special_case_2 = [{ORTH: \"#Hope\"}]\n",
    "nlp.tokenizer.add_special_case(\"Hunger Games\", special_case_1)\n",
    "nlp.tokenizer.add_special_case(\"#Hope\", special_case_2)\n",
    "doc = nlp(text_2)\n",
    "\n",
    "print(\"\\nSpecial case tokenization : \",end=' ')\n",
    "for token in doc:      # Checking new tokenization\n",
    "    print(token,end=', ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128cfb1b",
   "metadata": {},
   "source": [
    "## Text Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a92d3686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  Capital\n",
      "Transformed:  capital\n"
     ]
    }
   ],
   "source": [
    "word='Capital'\n",
    "print('Original: ',word)\n",
    "print('Transformed: ',word.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d03dbf",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "94f65571",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  ['Pipe', 'a', 'song', 'about', 'a', 'Lamb', '!\"', 'So', 'I', 'piped', 'with', 'merry', 'cheer', '.', '\"', 'Piper', ',', 'pipe', 'that', 'song', 'again', ';\"', 'So', 'I', 'piped', ':', 'he', 'wept', 'to', 'hear', '.', '\"', 'Drop', 'thy', 'pipe', ',', 'thy', 'happy', 'pipe', ';']\n",
      "Stemmed by porter:  ['pipe', 'a', 'song', 'about', 'a', 'lamb', '!\"', 'so', 'i', 'pipe', 'with', 'merri', 'cheer', '.', '\"', 'piper', ',', 'pipe', 'that', 'song', 'again', ';\"', 'so', 'i', 'pipe', ':', 'he', 'wept', 'to', 'hear', '.', '\"', 'drop', 'thi', 'pipe', ',', 'thi', 'happi', 'pipe', ';']\n"
     ]
    }
   ],
   "source": [
    "porter=nltk.PorterStemmer()\n",
    "lancaster=nltk.LancasterStemmer()\n",
    "poems=nltk.corpus.gutenberg.words('blake-poems.txt')[50:90]\n",
    "print('Original: ',poems)\n",
    "print('Stemmed by porter: ',[porter.stem(t) for t in poems])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ff762a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  ['Pipe', 'a', 'song', 'about', 'a', 'Lamb', '!\"', 'So', 'I', 'piped', 'with', 'merry', 'cheer', '.', '\"', 'Piper', ',', 'pipe', 'that', 'song', 'again', ';\"', 'So', 'I', 'piped', ':', 'he', 'wept', 'to', 'hear', '.', '\"', 'Drop', 'thy', 'pipe', ',', 'thy', 'happy', 'pipe', ';']\n",
      "Stemmed by lancaster:  ['pip', 'a', 'song', 'about', 'a', 'lamb', '!\"', 'so', 'i', 'pip', 'with', 'merry', 'che', '.', '\"', 'pip', ',', 'pip', 'that', 'song', 'again', ';\"', 'so', 'i', 'pip', ':', 'he', 'wept', 'to', 'hear', '.', '\"', 'drop', 'thy', 'pip', ',', 'thy', 'happy', 'pip', ';']\n"
     ]
    }
   ],
   "source": [
    "print('Original: ',poems)\n",
    "print('Stemmed by lancaster: ',[lancaster.stem(t) for t in poems])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1bdd4119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dog', 'have', 'lower', 'tradit']\n",
      "['dog', 'hav', 'low', 'tradit']\n"
     ]
    }
   ],
   "source": [
    "words = ['dogs','having','lower','traditional']\n",
    "print([porter.stem(w) for w in words])\n",
    "print([lancaster.stem(w) for w in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "abe334ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  ['Pipe', 'a', 'song', 'about', 'a', 'Lamb', '!\"', 'So', 'I', 'piped', 'with', 'merry', 'cheer', '.', '\"', 'Piper', ',', 'pipe', 'that', 'song', 'again', ';\"', 'So', 'I', 'piped', ':', 'he', 'wept', 'to', 'hear', '.', '\"', 'Drop', 'thy', 'pipe', ',', 'thy', 'happy', 'pipe', ';']\n",
      "Lemmatized:  ['Pipe', 'a', 'song', 'about', 'a', 'Lamb', '!\"', 'So', 'I', 'piped', 'with', 'merry', 'cheer', '.', '\"', 'Piper', ',', 'pipe', 'that', 'song', 'again', ';\"', 'So', 'I', 'piped', ':', 'he', 'wept', 'to', 'hear', '.', '\"', 'Drop', 'thy', 'pipe', ',', 'thy', 'happy', 'pipe', ';']\n"
     ]
    }
   ],
   "source": [
    "wnl=nltk.WordNetLemmatizer()\n",
    "print('Original: ',poems)\n",
    "print('Lemmatized: ',[wnl.lemmatize(t) for t in poems])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b0732d27",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  ['Pipe', 'a', 'song', 'about', 'a', 'Lamb', '!\"', 'So', 'I', 'piped', 'with', 'merry', 'cheer', '.', '\"', 'Piper', ',', 'pipe', 'that', 'song', 'again', ';\"', 'So', 'I', 'piped', ':', 'he', 'wept', 'to', 'hear', '.', '\"', 'Drop', 'thy', 'pipe', ',', 'thy', 'happy', 'pipe', ';']\n",
      "Lemmatized:  ['Pipe', 'a', 'song', 'about', 'a', 'Lamb', '!\"', 'So', 'I', 'pip', 'with', 'merry', 'cheer', '.', '\"', 'Piper', ',', 'pipe', 'that', 'song', 'again', ';\"', 'So', 'I', 'pip', ':', 'he', 'weep', 'to', 'hear', '.', '\"', 'Drop', 'thy', 'pipe', ',', 'thy', 'happy', 'pipe', ';']\n"
     ]
    }
   ],
   "source": [
    "print('Original: ',poems)\n",
    "print('Lemmatized: ',[wnl.lemmatize(t,'v') for t in poems])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e8496edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3be3d8dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n v a r\n"
     ]
    }
   ],
   "source": [
    "print(wn.NOUN,wn.VERB,wn.ADJ,wn.ADV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b071feb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('have', 'low')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(wnl.lemmatize('having','v'), wnl.lemmatize('lower','a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e8b5eab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([\"studying\",\"sciences\"], \"n\") --> ['studying', 'science']\n",
      "([\"studying\",\"sciences\"], \"v\") --> ['study', 'sciences']\n",
      "([\"studying\",\"sciences\"], both) --> ['study', 'science']\n"
     ]
    }
   ],
   "source": [
    "words=['studying','sciences']\n",
    "print('([\"studying\",\"sciences\"], \"n\") --> %s' % [wnl.lemmatize(x, 'n') \\\n",
    "                                                   for x in words])\n",
    "print('([\"studying\",\"sciences\"], \"v\") --> %s' % [wnl.lemmatize(x, 'v') \\\n",
    "                                                   for x in words])\n",
    "print('([\"studying\",\"sciences\"], both) --> %s' % \\\n",
    "      [wnl.lemmatize(wnl.lemmatize(x,'n'),'v') \\\n",
    "      for x in words])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81dd08e2",
   "metadata": {},
   "source": [
    "## Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a7a45bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import random\n",
    "#nltk.download('punkt')\n",
    "from nltk.corpus import reuters, stopwords\n",
    "from nltk import FreqDist, ngrams\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5f846652",
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = reuters.sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "afadd769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['ASIAN', 'EXPORTERS', 'FEAR', 'DAMAGE', 'FROM', 'U', '.', 'S', '.-', 'JAPAN', 'RIFT', 'Mounting', 'trade', 'friction', 'between', 'the', 'U', '.', 'S', '.', 'And', 'Japan', 'has', 'raised', 'fears', 'among', 'many', 'of', 'Asia', \"'\", 's', 'exporting', 'nations', 'that', 'the', 'row', 'could', 'inflict', 'far', '-', 'reaching', 'economic', 'damage', ',', 'businessmen', 'and', 'officials', 'said', '.'], ['They', 'told', 'Reuter', 'correspondents', 'in', 'Asian', 'capitals', 'a', 'U', '.', 'S', '.', 'Move', 'against', 'Japan', 'might', 'boost', 'protectionist', 'sentiment', 'in', 'the', 'U', '.', 'S', '.', 'And', 'lead', 'to', 'curbs', 'on', 'American', 'imports', 'of', 'their', 'products', '.'], ['But', 'some', 'exporters', 'said', 'that', 'while', 'the', 'conflict', 'would', 'hurt', 'them', 'in', 'the', 'long', '-', 'run', ',', 'in', 'the', 'short', '-', 'term', 'Tokyo', \"'\", 's', 'loss', 'might', 'be', 'their', 'gain', '.'], ['The', 'U', '.', 'S', '.', 'Has', 'said', 'it', 'will', 'impose', '300', 'mln', 'dlrs', 'of', 'tariffs', 'on', 'imports', 'of', 'Japanese', 'electronics', 'goods', 'on', 'April', '17', ',', 'in', 'retaliation', 'for', 'Japan', \"'\", 's', 'alleged', 'failure', 'to', 'stick', 'to', 'a', 'pact', 'not', 'to', 'sell', 'semiconductors', 'on', 'world', 'markets', 'at', 'below', 'cost', '.'], ['Unofficial', 'Japanese', 'estimates', 'put', 'the', 'impact', 'of', 'the', 'tariffs', 'at', '10', 'billion', 'dlrs', 'and', 'spokesmen', 'for', 'major', 'electronics', 'firms', 'said', 'they', 'would', 'virtually', 'halt', 'exports', 'of', 'products', 'hit', 'by', 'the', 'new', 'taxes', '.'], ['\"', 'We', 'wouldn', \"'\", 't', 'be', 'able', 'to', 'do', 'business', ',\"', 'said', 'a', 'spokesman', 'for', 'leading', 'Japanese', 'electronics', 'firm', 'Matsushita', 'Electric', 'Industrial', 'Co', 'Ltd', '&', 'lt', ';', 'MC', '.', 'T', '>.'], ['\"', 'If', 'the', 'tariffs', 'remain', 'in', 'place', 'for', 'any', 'length', 'of', 'time', 'beyond', 'a', 'few', 'months', 'it', 'will', 'mean', 'the', 'complete', 'erosion', 'of', 'exports', '(', 'of', 'goods', 'subject', 'to', 'tariffs', ')', 'to', 'the', 'U', '.', 'S', '.,\"', 'said', 'Tom', 'Murtha', ',', 'a', 'stock', 'analyst', 'at', 'the', 'Tokyo', 'office', 'of', 'broker', '&', 'lt', ';', 'James', 'Capel', 'and', 'Co', '>.'], ['In', 'Taiwan', ',', 'businessmen', 'and', 'officials', 'are', 'also', 'worried', '.'], ['\"', 'We', 'are', 'aware', 'of', 'the', 'seriousness', 'of', 'the', 'U', '.', 'S', '.'], ['Threat', 'against', 'Japan', 'because', 'it', 'serves', 'as', 'a', 'warning', 'to', 'us', ',\"', 'said', 'a', 'senior', 'Taiwanese', 'trade', 'official', 'who', 'asked', 'not', 'to', 'be', 'named', '.']]\n"
     ]
    }
   ],
   "source": [
    "print(sents[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e022af7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['through', 'under', 'itself', 'can', 'y', 'his', 'further', \"didn't\", 'some', 'doing', 'above', 'he', 'that', 'am', \"don't\", 'isn', 'was', \"haven't\", 'most', 'there', 'of', 'between', 'wouldn', 'those', 'each', 'yourselves', 'shouldn', \"wouldn't\", 'm', \"should've\", \"shouldn't\", 'to', \"you're\", 'very', 'theirs', 'mightn', 'because', 'd', 'needn', 'weren', 'it', 'until', 'so', 'both', 'not', 've', 'about', 'no', 'himself', 'don', 'are', 'during', 'whom', 'were', 'this', 'own', 'will', 'did', 'now', \"hasn't\", \"isn't\", 'against', 'ain', 'only', 'any', 'yourself', 'didn', 'haven', 'what', 'couldn', 'herself', 'its', 'be', 'i', 'having', 'up', 'again', 'on', 'wasn', 'being', 'ourselves', 'before', \"you'd\", 'in', 'o', 'few', 'too', 'hadn', \"mightn't\", 'out', 'than', 'him', 'hers', 'is', \"couldn't\", 'such', 'how', \"needn't\", \"you'll\", 'they', 'all', 'myself', 'down', 'does', 'after', 'at', 'from', 'hasn', 'themselves', 'which', 'ours', 'a', 'we', 'then', 'once', 'more', 'their', 'same', 'into', 'while', \"it's\", 'her', 'do', 'but', 'mustn', \"mustn't\", 'by', \"that'll\", 'our', 'just', 'been', \"aren't\", 'my', 'nor', 'if', 'had', 'for', 'll', 's', 'these', 'should', 'why', 'your', 'them', 'here', 'ma', 'the', 'aren', 'have', \"wasn't\", 'or', 'other', 'shan', 'off', \"won't\", \"weren't\", 'she', 'who', \"you've\", 'doesn', 'yours', \"shan't\", 're', 'won', 'and', 'me', \"doesn't\", 'has', 'with', 'an', 'you', \"hadn't\", \"she's\", 'over', 'where', 'below', 'when', 't', 'as', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~', '\"', '\"', '-', '+', 'â€”', 'lt', 'rt']\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "string.punctuation = string.punctuation +'\"' + '\"' + '-' + '''+''' + 'â€”'\n",
    "string.punctuation\n",
    "removal_list = list(stop_words) + list(string.punctuation)+ ['lt','rt']\n",
    "print(removal_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9a27b648",
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram=[]\n",
    "bigram=[]\n",
    "trigram=[]\n",
    "tokenized_text=[]\n",
    "for sentence in sents:\n",
    "    sentence = list(map(lambda x:x.lower(),sentence))\n",
    "    \n",
    "    # remove stop words and punctuation\n",
    "    sentence = [word for word in sentence if word not in removal_list]\n",
    "    tokenized_text.append(sentence)\n",
    "    unigram.extend(list(ngrams(sentence,1)))\n",
    "    bigram.extend(list(ngrams(sentence, 2,pad_left=True, pad_right=True)))\n",
    "    trigram.extend(list(ngrams(sentence, 3, pad_left=True, pad_right=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ed0c05fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigrams: \n",
      " [('asian',), ('exporters',), ('fear',), ('damage',), ('u',), ('.-',), ('japan',), ('rift',), ('mounting',), ('trade',)]\n",
      "\n",
      "Bigrams: \n",
      " [(None, 'asian'), ('asian', 'exporters'), ('exporters', 'fear'), ('fear', 'damage'), ('damage', 'u'), ('u', '.-'), ('.-', 'japan'), ('japan', 'rift'), ('rift', 'mounting'), ('mounting', 'trade')]\n",
      "\n",
      "Trigrams: \n",
      " [(None, None, 'asian'), (None, 'asian', 'exporters'), ('asian', 'exporters', 'fear'), ('exporters', 'fear', 'damage'), ('fear', 'damage', 'u'), ('damage', 'u', '.-'), ('u', '.-', 'japan'), ('.-', 'japan', 'rift'), ('japan', 'rift', 'mounting'), ('rift', 'mounting', 'trade')]\n"
     ]
    }
   ],
   "source": [
    "print(\"Unigrams: \\n\",unigram[:10])\n",
    "print(\"\\nBigrams: \\n\",bigram[:10])\n",
    "print(\"\\nTrigrams: \\n\",trigram[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "caf7c706",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_bi = FreqDist(bigram)\n",
    "freq_tri = FreqDist(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ded130ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common bigrams: \n",
      " [(('said', None), 7926), (('mln', 'dlrs'), 4401), (('mln', 'vs'), 3921), (('cts', 'vs'), 3311), (('000', 'vs'), 2581), (('cts', 'net'), 2194), ((None, 'said'), 2163), (('vs', 'loss'), 1780), (('billion', 'dlrs'), 1663), (('.\"', None), 1618)]\n",
      "\n",
      "Most common trigrams: \n",
      " [(('said', None, None), 7926), ((None, None, 'said'), 2163), (('.\"', None, None), 1618), (('dlrs', None, None), 1464), ((None, None, 'company'), 1278), (('year', None, None), 1110), ((None, None, 'u'), 1086), (('pct', None, None), 871), (('mln', None, None), 869), (('1986', None, None), 806)]\n"
     ]
    }
   ],
   "source": [
    "print('Most common bigrams: \\n',freq_bi.most_common(10))\n",
    "print('\\nMost common trigrams: \\n',freq_tri.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "78d1acdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_bi[('asian','exporters')],freq_tri[('asian','exporters','fear')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "608d775d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_word_probability(bi_g,w):\n",
    "    tri_g = bi_g + w\n",
    "    tri_g_freq, bi_g_freq = freq_tri[tri_g], freq_bi[bi_g]\n",
    "    prob_w = tri_g_freq / bi_g_freq if bi_g_freq > 0 else 0\n",
    "    return {w: prob_w}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "daa4e656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('fear',): 1.0}\n",
      "{('from',): 0.0}\n",
      "{('mounting',): 1.0}\n"
     ]
    }
   ],
   "source": [
    "print(next_word_probability(('asian', 'exporters'),('fear',)))\n",
    "print(next_word_probability(('asian', 'exporters'),('from',)))\n",
    "print(next_word_probability(('japan', 'rift'),('mounting',)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "51ee9b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_word(context):\n",
    "    all_scores = {}\n",
    "    for word in unigram:\n",
    "        word_score = next_word_probability(context,word)\n",
    "        all_scores.update(word_score)\n",
    "    item_max_value = max(all_scores.items(), key=lambda x: x[1])\n",
    "    list_of_keys = list()\n",
    "    # Iterate over all the items in dictionary to find keys with max value\n",
    "    for key, value in all_scores.items():\n",
    "        if value == item_max_value[1]:\n",
    "            list_of_keys.append(key)\n",
    "    return list_of_keys, item_max_value[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "018d7ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted words: \n",
      " [('mounting',)]\n",
      "\n",
      "Likelihood:  1.0\n"
     ]
    }
   ],
   "source": [
    "likely_words, probability = predict_next_word(('japan','rift'))\n",
    "print('Predicted words: \\n',likely_words)\n",
    "print('\\nLikelihood: ',probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5027960c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def continue_context(context,n_steps):\n",
    "    context_len=len(context)\n",
    "    assert context_len>=2, f\"context longer than 1 expected, got: {context_len}\"\n",
    "    print(context)\n",
    "    for i in range(n_steps):\n",
    "        short_context = context[-2:]\n",
    "        words,likelihood=predict_next_word(short_context)\n",
    "        if len(words)>1: \n",
    "            next_word = random.choice(words)\n",
    "        else:\n",
    "            next_word = words[0]\n",
    "        context += next_word\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a97b06e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('asian', 'exporters')\n"
     ]
    }
   ],
   "source": [
    "generated = continue_context(('asian', 'exporters'), 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "91b42267",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('asian', 'exporters', 'fear', 'china', 'may', 'lack', 'supplies', 'many', 'farmers', 'south', 'put', 'soybean', 'land', 'accounted', 'large', 'portion', 'positive', 'effects', 'growth', 'policy', 'statement', 'commissioner')\n"
     ]
    }
   ],
   "source": [
    "print(generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2ce0c9",
   "metadata": {},
   "source": [
    "## Hands On"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f9e1cb",
   "metadata": {},
   "source": [
    "Download the data from kaggle. \n",
    "https://www.kaggle.com/datasets/snap/amazon-fine-food-reviews\n",
    "\n",
    "You could also find a dataset you are interested in from the list on this page:\n",
    "https://github.com/niderhoff/nlp-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2cd8e238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load in the data\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"/Users/timowang/Downloads/archive/Reviews.csv\")\n",
    "texts = df[\"Text\"].to_list()\n",
    "sents = [word_tokenize(text.lower()) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f6e28785",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 568454/568454 [02:16<00:00, 4151.83it/s]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Normalize the text; Tokenize the texts into unigrams, bigrams and trigrams\n",
    "unigram=[]\n",
    "bigram=[]\n",
    "trigram=[]\n",
    "tokenized_text=[]\n",
    "for sentence in tqdm(sents):\n",
    "    sentence = list(map(lambda x:x.lower(),sentence))\n",
    "    \n",
    "    # remove stop words and punctuation containing non alphanum chars and br\n",
    "    sentence = [word for word in sentence if word not in removal_list and word.isalnum() and word != \"br\"]\n",
    "    tokenized_text.append(sentence)\n",
    "    unigram.extend(list(ngrams(sentence,1)))\n",
    "    bigram.extend(list(ngrams(sentence, 2,pad_left=True, pad_right=True)))\n",
    "    trigram.extend(list(ngrams(sentence, 3, pad_left=True, pad_right=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "874ca29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute frequencies of bigrams and trigrams\n",
    "freq_bi = FreqDist(bigram)\n",
    "freq_tri = FreqDist(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "dab676e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common bigrams: \n",
      " [((None, 'love'), 27889), ((None, 'great'), 15349), ((None, 'bought'), 15277), (('taste', 'like'), 14828), (('highly', 'recommend'), 14501), (('peanut', 'butter'), 13679), (('dog', 'food'), 12916), (('green', 'tea'), 12852), (('grocery', 'store'), 12368), (('tastes', 'like'), 11585)]\n",
      "\n",
      "Most common trigrams: \n",
      " [((None, None, 'love'), 27889), ((None, None, 'great'), 15349), ((None, None, 'bought'), 15277), (('product', None, None), 11257), ((None, None, 'tried'), 10106), ((None, None, 'like'), 10053), ((None, None, 'first'), 8930), ((None, None, 'really'), 8788), ((None, None, 'product'), 8694), (('buy', None, None), 8461)]\n"
     ]
    }
   ],
   "source": [
    "print('Most common bigrams: \\n',freq_bi.most_common(10))\n",
    "print('\\nMost common trigrams: \\n',freq_tri.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ff1b8e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted words: [('also',)] \n",
      "\n",
      "Likelihood: 0.291497975708502 \n",
      "\n",
      "Predicted words: [('sauce',), ('mushrooms',), ('rub',)] \n",
      "\n",
      "Likelihood: 0.2222222222222222 \n",
      "\n",
      "Predicted words: [('vinegar',)] \n",
      "\n",
      "Likelihood: 0.23741007194244604 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: Compute the most likely words for the following bigrams: white wine, None steak, red white\n",
    "\n",
    "likely_words, probability = predict_next_word(('white','wine'))\n",
    "print(f'Predicted words: {likely_words} \\n')\n",
    "print(f'Likelihood: {probability} \\n')\n",
    "\n",
    "likely_words, probability = predict_next_word((None,'steak'))\n",
    "print(f'Predicted words: {likely_words[:20]} \\n')\n",
    "print(f'Likelihood: {probability} \\n')\n",
    "\n",
    "likely_words, probability = predict_next_word(('red','wine'))\n",
    "print(f'Predicted words: {likely_words[:20]} \\n')\n",
    "print(f'Likelihood: {probability} \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ecc571e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('white', 'wine')\n",
      "('white', 'wine', 'also', 'moderate', 'amount', 'acid', 'subdue', 'strong', 'flavors', 'ginger', 'lime', 'garlic', 'cilantro', 'without', 'classically', 'riesling', 'fits', 'parameters', 'perfectly', 'also', 'recommend', 'http')\n",
      "('red', 'wine')\n",
      "('red', 'wine', 'vinegar', 'help', 'ended', 'sitting', 'refrigerator', '6', 'months', 'ago', 'started', 'feeding', 'cat', 'food', 'protein', 'followed', 'carbohydrate', 'first', 'two', 'ingredients', 'science', 'diet')\n",
      "(None, 'steak')\n",
      "(None, 'steak', 'rub', 'answered', 'quickly', 'said', 'sending', 'bag', 'smaller', 'expected', 'price', 'also', 'great', 'price', 'amazon', 'much', 'better', 'price', 'amazon', 'much', 'better', 'price')\n"
     ]
    }
   ],
   "source": [
    "# TODO: Generate texts of length 20 for the following contexts: white wine, None steak, red white\n",
    "generated = continue_context(('white', 'wine'), 20)\n",
    "print(generated)\n",
    "\n",
    "generated = continue_context(('red', 'wine'), 20)\n",
    "print(generated)\n",
    "\n",
    "generated = continue_context((None, 'steak'), 20)\n",
    "print(generated)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
