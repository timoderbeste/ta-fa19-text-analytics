{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d99c2524",
   "metadata": {},
   "source": [
    "# Lab 3 - Pretrained language models and text classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7153ac1",
   "metadata": {},
   "source": [
    "## Classification with language models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78212388",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed08efdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-20 18:12:01.570678: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model, Input \n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "tfds.disable_progress_bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "affb58c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a015169",
   "metadata": {},
   "source": [
    "### Download the data\n",
    "Find a proper dataset [here](https://www.tensorflow.org/datasets/catalog/).\n",
    "Download the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da635e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(TensorSpec(shape=(), dtype=tf.string, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-20 18:12:05.064364: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# TODO: Download the dataset with tensorflow dataset and obtain train and test datasets.\n",
    "\n",
    "dataset, info = tfds.load(\"yelp_polarity_reviews\", with_info=True, as_supervised=True)\n",
    "train_dataset, test_dataset = dataset[\"train\"], dataset[\"test\"]\n",
    "\n",
    "print(train_dataset.element_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1ea8fa",
   "metadata": {},
   "source": [
    "### Prepare the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ecf37c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Choose your transformer model and specify your model_path\n",
    "\n",
    "MODEL = \"distilbert-base-uncased\"\n",
    "\n",
    "model_path = f\"/Users/timowang/Daten/models/lab3/{MODEL}\"\n",
    "\n",
    "from transformers import AutoTokenizer, TFAutoModel, logging\n",
    "folder = os.fspath(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15af8d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['vocab_layer_norm', 'vocab_projector', 'activation_13', 'vocab_transform']\n",
      "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('/Users/timowang/Daten/models/lab3/distilbert-base-uncased/tokenizer_config.json',\n",
       " '/Users/timowang/Daten/models/lab3/distilbert-base-uncased/special_tokens_map.json',\n",
       " '/Users/timowang/Daten/models/lab3/distilbert-base-uncased/vocab.txt',\n",
       " '/Users/timowang/Daten/models/lab3/distilbert-base-uncased/added_tokens.json',\n",
       " '/Users/timowang/Daten/models/lab3/distilbert-base-uncased/tokenizer.json')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Load and save pretrained transformer model and tokenizer.\n",
    "\n",
    "transformer = TFAutoModel.from_pretrained(MODEL)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "transformer.save_pretrained(folder)\n",
    "tokenizer.save_pretrained(folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25774a2",
   "metadata": {},
   "source": [
    "### Process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5a22036",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_data(data, max_len):\n",
    "    enc_data = tokenizer(data,max_length=max_len, return_token_type_ids=False,\n",
    "                         padding=True, truncation=True)\n",
    "    return [np.array(enc_data[k]) for k in ['input_ids','attention_mask']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e319e478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Encode train and test texts with the function `encode_data`\n",
    "\n",
    "train_texts = []\n",
    "train_labels = []\n",
    "for text, label in train_dataset.take(10000):\n",
    "    train_texts.append(text.numpy().decode(\"utf-8\"))\n",
    "    train_labels.append(label.numpy())\n",
    "    \n",
    "test_texts = []\n",
    "test_labels = []\n",
    "for text, label in test_dataset.take(1000):\n",
    "    test_texts.append(text.numpy().decode(\"utf-8\"))\n",
    "    test_labels.append(label.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69855c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 64\n",
    "\n",
    "train_data = encode_data(train_texts, MAX_LEN)\n",
    "test_data = encode_data(test_texts, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b640e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the encoded data:  (10000, 64)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of the encoded data: ', train_data[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40a31cac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 3s 3s/step\n",
      "<class 'transformers.modeling_tf_outputs.TFBaseModelOutput'> 1 (2, 64, 768)\n"
     ]
    }
   ],
   "source": [
    "# TODO: Try encode a few pieces of texts and examine the output shape\n",
    "\n",
    "sample_train_data = encode_data(train_texts[:2], MAX_LEN)\n",
    "output = transformer.predict(sample_train_data)\n",
    "print(type(output),len(output),output[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67510d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_data\n",
    "X_test = test_data\n",
    "y_train = train_labels\n",
    "y_test = test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1877f86c",
   "metadata": {},
   "source": [
    "### Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f82d0939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define your model here. Depend on your dataset, \n",
    "#   you may need to use a different activation function for the last layer. \n",
    "#   You may also need to use a different loss function other than binary_crossentropy.\n",
    "\n",
    "def get_model(transformer):\n",
    "    input_word_ids = Input(shape=(MAX_LEN,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "    input_att_mask = Input(shape=(MAX_LEN,), dtype=tf.int8, name=\"input_att_mask\")\n",
    "    transformer.trainable = False\n",
    "    x = transformer([input_word_ids,input_att_mask])[0][:, 0, :]  \n",
    "    out = Dense(1, activation='sigmoid',name = 'custom_dense')(x)\n",
    "    model = Model(inputs=[input_word_ids,input_att_mask], outputs=out)\n",
    "    model.compile(loss=\"binary_crossentropy\", \n",
    "                  optimizer=Adam(learning_rate=1e-3), metrics=[\"acc\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99741eb5",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bee0fe50",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_word_ids (InputLayer)    [(None, 64)]         0           []                               \n",
      "                                                                                                  \n",
      " input_att_mask (InputLayer)    [(None, 64)]         0           []                               \n",
      "                                                                                                  \n",
      " tf_distil_bert_model (TFDistil  TFBaseModelOutput(l  66362880   ['input_word_ids[0][0]',         \n",
      " BertModel)                     ast_hidden_state=(N               'input_att_mask[0][0]']         \n",
      "                                one, 64, 768),                                                    \n",
      "                                 hidden_states=None                                               \n",
      "                                , attentions=None)                                                \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_distil_bert_model[0][0]']   \n",
      " ingOpLambda)                                                                                     \n",
      "                                                                                                  \n",
      " custom_dense (Dense)           (None, 1)            769         ['tf.__operators__.getitem[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 66,363,649\n",
      "Trainable params: 769\n",
      "Non-trainable params: 66,362,880\n",
      "__________________________________________________________________________________________________\n",
      "Model: classifier head traning only: \n",
      " None\n"
     ]
    }
   ],
   "source": [
    "best_model_path = 'best_model_head.h5'\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# save the best model to a file\n",
    "chp = ModelCheckpoint(best_model_path,save_best_only=True,save_weights_only=True)\n",
    "model = get_model(transformer)\n",
    "print('Model: classifier head traning only: \\n',model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "464b705d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "250/250 [==============================] - 258s 1s/step - loss: 0.5545 - acc: 0.7314 - val_loss: 0.4790 - val_acc: 0.7870\n",
      "Epoch 2/3\n",
      "250/250 [==============================] - 265s 1s/step - loss: 0.4621 - acc: 0.7918 - val_loss: 0.4331 - val_acc: 0.8070\n",
      "Epoch 3/3\n",
      "250/250 [==============================] - 269s 1s/step - loss: 0.4372 - acc: 0.7985 - val_loss: 0.4164 - val_acc: 0.8160\n"
     ]
    }
   ],
   "source": [
    "best_model_path = 'best_model_whole.h5'\n",
    "chp = ModelCheckpoint(best_model_path, save_best_only=True, save_weights_only=True)\n",
    "    \n",
    "\n",
    "hist = model.fit(X_train, np.array(y_train),\n",
    "                 validation_split=0.2,\n",
    "                 epochs=3, shuffle=True, callbacks=[chp],\n",
    "                 verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eb6de458",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.load_weights(best_model_path)\n",
    "test_pred = model.predict(X_test, batch_size=BATCH_SIZE, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df328577",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC after head finetuning 0.8964935023365982\n"
     ]
    }
   ],
   "source": [
    "print('AUC after head finetuning',roc_auc_score(y_test, test_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
